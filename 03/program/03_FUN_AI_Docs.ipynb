{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三回\n",
    "今日は、全結合層を重ねたシンプルな深層学習ネットワークを用いて、手書き文字認識を行い、Kaggle に提出をしてみます。\n",
    "\n",
    "今回、チャレンジするコンペは、[digit-recognizer](https://www.kaggle.com/c/digit-recognizer/) という常設のチュートリアルのようなものです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "全結合層のみで構成されているやつ。\n",
    "\n",
    "![](../image/02.png)\n",
    "\n",
    "今回は、MLPを用いて、クラス分類をしていきます。データセットは[ここ](https://www.kaggle.com/c/digit-recognizer/data)からダウンロードしてください\n",
    "\n",
    "![](../image/03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU function\n",
    "正規化線形関数  \n",
    "近年の深層学習で、中間層の活性化関数として主に使われている。\n",
    "$$ \\mathrm{ReLU}(x) = \\max(0, x)$$ で表される関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax function\n",
    "活性化関数の1つ。  \n",
    "特徴として\n",
    "- 合計が1ですべて正　(確率として使える)\n",
    "- sigmoid 関数の多変数拡張  \n",
    "というものがある。sigmoid云々の話はPRMLにあるので、そちらを見てください。(PRML 上巻 p196)\n",
    "\n",
    "名前の由来はソフト(滑らかな)マックス関数。該当クラスの確率が極端に高くなることから、多クラス分類の最後に使われることが多い。\n",
    "\n",
    "定義式：  \n",
    "\n",
    "$$ \\mathrm{softmax}(\\mathbb{a}) = \\frac{\\exp(a_k)}{\\sum_j \\exp(a_j)} $$\n",
    "\n",
    "[参考](https://www.youtube.com/watch?v=5CwLT-IQB9E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"../input/digit-recognizer/train.csv\", dtype=np.float32)\n",
    "\n",
    "# split data into features(pixels) and labels(numbers from 0 to 9)\n",
    "# ラベル(0-9)\n",
    "targets_numpy = train.label.values\n",
    "# 画像データセット (784x1に伸ばされている)\n",
    "features_numpy = train.loc[:, train.columns != \"label\"].values/255\n",
    "\n",
    "# 訓練用とテスト用にデータセットを分割\n",
    "features_train, features_test, targets_train, targets_test = train_test_split(\n",
    "    features_numpy, targets_numpy, test_size=0.2, random_state=42)\n",
    "\n",
    "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "# NumPy配列からTensorに変換\n",
    "featuresTrain = torch.from_numpy(features_train)\n",
    "targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# create feature and targets tensor for test set.\n",
    "# NumPy配列からTensorに変換\n",
    "featuresTest = torch.from_numpy(features_test)\n",
    "targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# batch_size, epoch and iteration\n",
    "# バッチサイズとエポック数を決定\n",
    "batch_size = 100\n",
    "n_iters = 10000\n",
    "num_epochs = n_iters / (len(features_train) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "# Pytorch train and test sets\n",
    "# 画像とラベルの組をデータセット に変換\n",
    "train = TensorDataset(featuresTrain,targetsTrain)\n",
    "test = TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "# data loader\n",
    "# ミニバッチ用のデータローダーを作成\n",
    "train_loader = DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# visualize one of the images in data set\n",
    "# 画像を可視化\n",
    "plt.imshow(features_numpy[10].reshape(28,28))\n",
    "plt.axis(\"off\")\n",
    "plt.title(str(targets_numpy[10]))\n",
    "plt.savefig('graph.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力は28*28 の 784次元\n",
    "input_dim = 28*28\n",
    "# 出力は0-9 の10次元\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワーク定義。今回は３層のMLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        # ネットワーク定義\n",
    "        self.mlp = nn.Sequential(\n",
    "            # 全結合層\n",
    "            nn.Linear(input_dim, 256),\n",
    "            # 活性化関数\n",
    "            nn.ReLU(),\n",
    "            # 全結合層\n",
    "            nn.Linear(256, 256),\n",
    "            # 活性化関数\n",
    "            nn.ReLU(),\n",
    "            # 全結合層\n",
    "            nn.Linear(256, output_dim),\n",
    "            # 活性化関数\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(input_dim, output_dim)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価関数\n",
    "def eval_net(net, data_loader, device=\"cpu\"):\n",
    "    net.eval()\n",
    "    labels = []\n",
    "    labels_preds= []\n",
    "    for image, label in data_loader:\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        with torch.no_grad():\n",
    "            _, label_pred = net(image).max(1)\n",
    "        labels.append(label)\n",
    "        labels_preds.append(label_pred)\n",
    "\n",
    "    labels = torch.cat(labels)\n",
    "    labels_preds = torch.cat(labels_preds)\n",
    "\n",
    "    # CSVファイルを吐き出さなければいけない\n",
    "    # 推論したラベルはlabelsに入ってる。画像のIDは知らない\n",
    "\n",
    "    acc = (labels == labels_preds).float().sum() / len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練用の関数\n",
    "def train_net(net, train_loader, test_loader, optimizer, loss_fn, n_iters=10, device=\"cpu\"):\n",
    "    # \n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    for epoch in range(n_iters):\n",
    "        running_loss = 0.\n",
    "        net.train()\n",
    "        n = 0\n",
    "        srore = 0\n",
    "\n",
    "        for i, (images, labels) in tqdm(enumerate(train_loader),                    total=len(train_loader)):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            label_pred = net(images)\n",
    "            loss = loss_fn(label_pred, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            n += len(images)\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        # train_acc.append(n_a)\n",
    "        val_acc.append(eval_net(net, test_loader, device))\n",
    "        print(epoch, train_losses[-1], val_acc[-1], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# データをGPUに渡す\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "net.to(device)\n",
    "\n",
    "# 訓練を実行\n",
    "train_net(net, train_loader, test_loader, optimizer=optimizer, loss_fn=loss_fn, n_iters=20, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証用データセットを作成\n",
    "test = pd.read_csv(r\"../input/digit-recognizer/test.csv\", dtype=np.float32)\n",
    "\n",
    "# データを正規化し、Tensorを作る\n",
    "features_test = test.values/255\n",
    "features_test = torch.from_numpy(features_test)\n",
    "test = TensorDataset(features_test)\n",
    "\n",
    "# データローダを作成\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論用の関数\n",
    "def prediction(data_loader, device='cpu'):\n",
    "    net.eval()\n",
    "    test_pred = torch.LongTensor()\n",
    "\n",
    "    for i, images in enumerate(data_loader):\n",
    "        # print(images[0].size())\n",
    "        images = images[0].to(device)\n",
    "        output = net(images)\n",
    "        _, pred = output.cpu().data.max(1, keepdim=True)\n",
    "        test_pred = torch.cat((test_pred, pred), dim=0)\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論を行う\n",
    "test_pred = prediction(test_loader, device=device)\n",
    "\n",
    "# データの整形\n",
    "out_df = pd.DataFrame(np.c_[np.arange(1, len(test)+1)[:,None],\n",
    "    test_pred.numpy()], columns=['ImageId', 'Label'])\n",
    "\n",
    "# 出力\n",
    "out_df.head()\n",
    "out_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bitpytorchcondac1588a0183c145a7ad887ad140edf980",
   "display_name": "Python 3.7.5 64-bit ('pytorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}